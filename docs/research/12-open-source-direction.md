# 12. Open Source AI Accountability: Direction Document

**Status:** Draft
**Created:** November 28, 2025

---

## The Pivot

Previous research explored enterprise-focused models (Research Firm, Certification Body). This document explores a different direction: an open source, community-driven approach to AI accountability.

**Why the shift:**
- Enterprise/compliance angle doesn't fit founder skills or interests
- Requires regulatory expertise Tim doesn't have
- Feels inauthentic to the core motivation

---

## Core Philosophy

### What This Is

**Pro-AI, pro-informed-decisions.**

Not anti-AI. Not a watchdog that attacks. An organization that:
1. Helps people understand what they're using
2. Makes information public and accessible
3. Through transparency, pushes companies toward better practices
4. Lets people make up their own minds

### What Makes It Different

1. **Radical transparency** - Everything is open source: research, data, methodology, even the code that generates outputs
2. **Self-accountability** - Publish our own AI token usage, costs, environmental footprint
3. **No ideological gate** - Contributors can be pro-AI or skeptical; profiles show perspectives so readers can judge
4. **Invitation over accusation** - "Here's what we found, here's how we did it, help us improve"

---

## Structure

### Open Source Foundation

Everything on GitHub:
- Research methodology
- Data collection scripts
- Analysis code
- Raw data (where possible)
- Website/publishing infrastructure

### Contributor Model

**Core team:** Tim + volunteers initially

**Volunteer sources:**
- Superpath community (content marketers interested in AI ethics)
- Engineers who care about responsible AI
- Researchers, students, journalists
- Anyone who wants to contribute

**Contributor profiles:**
- Each contributor has a public profile
- States their perspective on AI (pro, skeptical, neutral, specific concerns)
- Links to their contributions
- Readers can factor this into how they interpret work

### Transparency Infrastructure

**Self-reporting:**
- Token usage for all research published on the website
- Cost of producing each piece of content
- Environmental estimates where possible

**Automated publishing:**
- Claude Code infrastructure to auto-publish metrics
- GitHub Actions or similar for continuous deployment
- Everything auditable

---

## Types of Outputs

### Comparative Research

**Example: Opus vs Haiku Environmental Footprint**
- What's the actual difference in compute/energy between models?
- Should there be a "green option" indicator?
- Desk research → methodology published → invite contributions

**Example: Training Data Transparency Comparison**
- Which companies disclose training data sources?
- What do they disclose vs. hide?
- Factual comparison, not judgment

### Company Profiles

Simple, factual profiles:
- What do they disclose publicly?
- What policies do they have?
- What have they been criticized for?
- What have they improved?

Not "ratings" necessarily - just organized, accessible information.

### Methodology Documentation

- How we assess things
- What data sources we use
- Limitations and caveats
- Version history of methodology

### Self-Accountability Reports

- Monthly/quarterly reports on our own AI usage
- What we spent, what we used, why
- Walking the talk

---

## Income Paths

### Phase 1: Side Project (No Income)

- Volunteer contributions
- Tim's time as investment
- Zero overhead (GitHub, free hosting)
- Build credibility and audience

### Phase 2: Sustainable Side Project

**Donations/Sponsorships:**
- Open Collective or GitHub Sponsors
- "Support independent AI accountability research"
- Companies that want to signal commitment (risky - independence concerns)

**Grants:**
- Foundation funding for public interest tech
- Academic partnerships
- Journalism grants (if investigative angle develops)

**Target:** Cover costs, maybe small stipends for core contributors

### Phase 3: Transition to Income

**If traction proves demand:**

Option A: **Newsletter/Premium Content**
- Free: All research, data, methodology
- Paid: Analysis, synthesis, "what this means for you"
- Similar to open source software with paid support

Option B: **Consulting/Advisory**
- Companies pay for help understanding/improving their practices
- Independence risk - would need clear policies

Option C: **Grants + Donations at Scale**
- Some nonprofits sustain on this (Wikipedia, Mozilla)
- Requires significant audience/reputation

Option D: **Hybrid**
- Mix of above based on what works

**Key principle:** Core research always free and open. Income from value-add, not from gating information.

---

## Differentiation

### What Exists Today

| Organization | Approach | Limitation |
|--------------|----------|------------|
| SaferAI | Academic ratings | 20 companies, closed methodology |
| Stanford FMTI | Foundation model index | 14 companies, academic audience |
| Various journalists | Investigative pieces | One-off, not systematic |
| AI Now, Partnership on AI | Policy/research | Academic, not consumer-facing |

### What This Would Be

| Attribute | This Project |
|-----------|--------------|
| Methodology | Fully open source |
| Data | Publicly available |
| Audience | Anyone (consumers, companies, researchers) |
| Tone | Informative, not accusatory |
| Self-accountability | Publish own AI footprint |
| Contributor model | Open, with transparent perspectives |
| Coverage | Start narrow, expand based on contributions |

**Unique positioning:** "The only AI accountability project that holds itself to the same standard it holds others."

---

## Evolution Path

### Month 1-3: Foundation

- [ ] Set up GitHub infrastructure
- [ ] Create website (simple, static)
- [ ] Define first research project (e.g., model environmental comparison)
- [ ] Publish first piece with full methodology
- [ ] Set up token tracking and self-reporting
- [ ] Recruit 2-3 initial contributors

### Month 4-6: Prove the Model

- [ ] 3-5 published research pieces
- [ ] Methodology documentation complete
- [ ] Contributor profiles live
- [ ] Self-accountability reports published
- [ ] Initial audience building (Twitter, Superpath, HN)

### Month 7-12: Build Reputation

- [ ] Expand coverage
- [ ] Seek initial grants or donations
- [ ] Grow contributor base
- [ ] Establish as credible source
- [ ] Decision point: Is there traction?

### Year 2+: If Traction

- Explore income options
- Potentially transition Tim to part-time → full-time
- Scale contributor model
- Consider formal nonprofit structure

---

## Risks and Mitigations

### Risk: No One Contributes

**Mitigation:** Start with what Tim can do alone. Contributions are upside, not requirement.

### Risk: Companies Attack Credibility

**Mitigation:** Radical transparency makes attacks harder. Everything is auditable. Invite corrections publicly.

### Risk: Independence Compromised by Funding

**Mitigation:**
- Disclose all funding sources
- No funding from companies being covered (or strict policies if so)
- Prioritize grants/donations over corporate sponsors

### Risk: Burnout (Unpaid Side Project)

**Mitigation:**
- Keep scope narrow initially
- Don't over-commit
- Set explicit "pause" criteria if unsustainable

### Risk: Legal Issues (Defamation, etc.)

**Mitigation:**
- Focus on facts, not accusations
- Everything caveated and sourced
- Publish corrections promptly
- Consider media liability insurance eventually

---

## Open Questions

1. **Name/Brand:** Is "Fair AI" right for this direction? Alternatives?
   - Open AI Watch
   - AI Transparency Project
   - Clear AI
   - Something else?

2. **First Project:** Environmental comparison (Opus vs Haiku) or something else?
   - Training data transparency?
   - Labor practices comparison?
   - Privacy policy comparison?

3. **Legal Structure:** Eventually need formal structure
   - 501(c)(3) nonprofit?
   - Open Collective fiscal sponsorship initially?
   - Just personal project to start?

4. **Scope:** How narrow to start?
   - Just foundation models (OpenAI, Anthropic, Google, Meta)?
   - Broader AI tools?
   - Specific use cases?

5. **Contributor Vetting:** How open is "open"?
   - Anyone can submit?
   - Review process?
   - Core team approval?

---

## Why This Could Work

1. **Timing:** AI accountability is a growing concern, but no open source player exists
2. **Founder fit:** Matches Tim's skills (content, research, AI fluency) and values
3. **Low stakes to start:** Side project, volunteers, no income pressure initially
4. **Defensible:** Hard to attack something that's fully transparent
5. **Scalable:** Contributor model can grow without Tim doing everything
6. **Unique angle:** Self-accountability differentiates from "do as I say not as I do" critics

---

## Why This Could Fail

1. **No traction:** Nobody cares, no audience, no contributors
2. **Never monetizes:** Stays a hobby forever, never replaces income
3. **Someone else does it better:** Well-funded org launches similar project
4. **Burnout:** Unpaid work becomes unsustainable
5. **Credibility never established:** Seen as amateur effort, not taken seriously

---

## Next Steps

1. Decide: Does this direction feel right?
2. If yes: Answer open questions (name, first project, structure)
3. Set up basic infrastructure (GitHub org, simple website)
4. Define and execute first research project
5. Publish and see what happens

---

## Comparison to Previous Research

| Dimension | Research Firm Model | Open Source Model |
|-----------|--------------------|--------------------|
| Founder fit | Moderate | High |
| Income timeline | 18-24 months | Uncertain (could be longer) |
| Startup cost | High (team, runway) | Low (side project) |
| Risk | Financial | Time/opportunity cost |
| Scalability | Hire people | Recruit contributors |
| Differentiation | Methodology depth | Radical transparency |
| Impact mechanism | Enterprise procurement | Public awareness + shame |
| Independence | Client pressure risk | Funding pressure risk |

**Trade-off:** Lower financial risk, higher uncertainty about income timeline. But more authentic to founder and lower barrier to start.
