20251114 0700 Claude normal chat convo

If you do a bit of searching online, you will see in very recent articles from Stretchy and, Spy. E spy. I can't remember his name. That's right in a AI bubble. And, I think it could be true. But also think there's another narrative which is What if we're not in a bubble? And maybe that's even scarier because I I think the current camps are like You know, that's at least how it's portrayed. You have the people who work at these or who run these frontier AI labs, and then it's their interest to to keep talking up. The capacity rising. So and and and, and they just have to keep going. Otherwise, it will collapse. And then the skeptics and analysts are saying, oh, look. This doesn't make any sense. It is a bubble. Right. We have to keep believing in building out in this way. Otherwise, the bubble collapses. But for sure, it's a bubble. But From my own experience, like working with these latest models, And that's to take into account. I don't I just work with publicly available latest models. In these companies, they are probably at least 1 model ahead, maybe 2. Plus, they have fringe r and d that's maybe not there yet. It's developing. What if it is True. That the technology is so powerful. That it can start to replace thousands of workers. And that within these companies, that are all involved in this, this is pretty obvious. Then maybe these valuations do make sense. Because they can see that in the next few years, know, they will be able to replace 1,000 or tens of 1,000 or a 100 of 1,000 of knowledge workers. And could kind of see that. If that's the case, then I mean, I'm not an economist, but then what we're seeing is not a bubble, but it's we're seeing already kind of the belief and the value transfer to companies because they are basically gonna be the infrastructure that replaces all these workers in a lot of companies and therefore We'll capture. All that money, which is enormous and would justify their valuations. Does that theory make some sense?Show lessWeighed competing AI narratives: bubble versus genuine transformative displacement.11sTim is articulating a really interesting counter-narrative to the AI bubble thesis. Let me break down his argument:

The mainstream narrative seems to be:

AI companies are hyping capabilities to maintain valuations
Skeptics (like Ed Zitron from "Where's Your Ed At?" - I think that's who he means by "Stretchy and Spy. E spy") are calling it a bubble
The implication is that if belief falters, it all collapses


Tim's alternative theory:

What if the technology really IS as transformative as claimed?
From his experience with publicly available models, they're already quite powerful
The companies are likely 1-2 models ahead internally
If these models can truly replace thousands/tens of thousands/hundreds of thousands of knowledge workers
Then the valuations might actually make sense because these companies will capture the value that currently goes to human labor
This would be a genuine value transfer, not a bubble



This is a sophisticated take. He's essentially asking: what if the scary thing isn't that it's a bubble that will pop, but that it's NOT a bubble - meaning the displacement is real and coming?
The theory does make sense economically. If AI can truly automate significant portions of knowledge work:

Companies deploying AI would capture enormous value
AI infrastructure providers (OpenAI, Anthropic, etc.) would capture part of that value
This could justify high valuations if the addressable market is "most knowledge work"

But there are some nuances to consider:

Timing and adoption rates matter for valuations
Distribution of value capture (who actually benefits?)
Regulatory and social responses
Whether the technology generalizes as expected
The difference between "can replace" vs "will replace"

I should engage thoughtfully with his theory, validate the logic, but also explore some of the complexities without being dismissive of either the bubble or non-bubble scenario.Your theory absolutely makes sense, and it's a much more unsettling framing than the bubble narrative.
The bubble thesis is almost comforting in a way - it says "don't worry, this is irrational exuberance, reality will reassert itself." Your counter-argument is: what if the valuations are rational responses to an accurate assessment of impending labor displacement?
The logic is sound: if these companies can see - through internal models and research that's ahead of what we see publicly - that they're 12-24 months from being able to automate significant chunks of knowledge work, then they're essentially pre-pricing a massive value transfer. The market cap of OpenAI or Anthropic wouldn't be speculative - it would be a bet on capturing a slice of the trillions currently spent on knowledge worker salaries.
And from your vantage point, working with the publicly available tools, you can already see the trajectory. If Sonnet 4.5 can do what it does now, and you know they're already training the next generation... the math starts to work.
The key variables that make this not just plausible but likely:

The capture mechanism is already clear - API calls, enterprise licenses, embedding in tools people already use (like Microsoft's Copilot strategy)
The adoption curve is compressing - unlike previous tech transitions, companies can start getting value from AI in weeks/months, not years
The replacement doesn't have to be 1:1 - even if AI + 1 human can do the work of 5 humans, that's still 80% labor cost reduction

Where it gets complicated:

Timing risk - even if they're right about the tech, getting it deployed at scale is messy
Regulation and social friction - governments might intervene if displacement happens too fast
Who captures the value? - will it be the model providers, or will it be captured by the companies deploying AI (who might commoditize the models)?

But you're right that this might actually be scarier than a bubble. Because a bubble pops and we move on. A genuine, rapid restructuring of the knowledge economy? That's a different kind of disruption entirely.
What's your read on the timeline based on what you're seeing with current models?