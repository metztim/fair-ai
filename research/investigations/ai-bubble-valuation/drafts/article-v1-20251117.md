# Let’s hope we’re in an AI bubble, the alternative is worse

Many analysts agree we're in an AI bubble. Here's how M.G. Siegler of Spyglass opened [a recent article](https://spyglass.org/ai-bubble/?ref=spyglass-newsletter):

> _"Yes, it's a bubble. Yes, that may be a good thing. Yes, it will hurt."_

Ben Thompson of Stratechery is [even more blunt](https://stratechery.com/2025/the-benefits-of-bubbles/): "_this_ is a bubble."

The current AI boom indeed checks all bubble boxes:

1. The companies involved are overvalued. (e.g., OpenAI at [$500B valuation](https://sacra.com/c/openai/) with $20B in annual recurring revenue.)
2. The deals they're making are crazy. (e.g., OpenAI signing [$1-1.5T in vendor deals](https://www.theregister.com/2025/11/04/the_circular_economy_of_ai/), often in complex equity-for-service structures, like [granting AMD warrants](https://ir.amd.com/financial-information/sec-filings/content/0001193125-25-230895/d28189d8k.htm) for up to 10% of the company at $0.01/share in exchange for $90-100B in chip purchases.)
3. The current technology disappoints.

This last point is the core of the argument. If AI _was_ delivering on its promise, the valuations and deals would be justified, so we wouldn't be in a bubble.

## Does AI live up to its overhyped promise?

[MIT NANDA found](https://virtualizationreview.com/articles/2025/08/19/mit-report-finds-most-ai-business-investments-fail-reveals-genai-divide.aspx) that 95% of generative AI investments have produced no measurable returns. Most companies — outside of tech — have anyway barely begun using AI: [only 9.7% of US firms](https://www.anthropic.com/research/anthropic-economic-index-september-2025-report) had adopted AI as of August 2025. And it's easy to find published examples of AI's gaffes: lawyers using hallucinated "evidence," images with too many fingers, and far worse examples of bias, racism, and other sins.

But what if those indicators of AI’s performance are incorrect, or, more precisely, outdated?

My AI journey has been [riddled with disappointment](https://www.animalz.co/blog/ai-addiction). But over the past three to six months, things have changed: workflows I dreamed up since ChatGPT’s release are finally working, often beyond my original wild visions for them.

For example, for an app development side project, Claude Code now writes the code, corrects its own UX design by using Chrome, and manages its own tasks and priorities on a Notion product board.

My role in the project is best described as orchestrator, taste maker, or master chef — even product manager would oversell my contributions.

I’ve also advised companies where _at least_ 25% of work previously done by humans is now completely replaceable by AI. In some, it’s over 50%.

This includes work in finance, HR, operations, and, to an extend, coding and data analysis.[1](#fn:1) Soon, I believe AI can also handle a significant part of work done by middle managers.

This assessment is not based on forecasts or claims by others. I’m experiencing first-hand what a capable model (Sonnet 4.5) combined with unique data sources[2](#fn:2) and great tools (e.g., Claude Code) can do.

## The Four Year AI Performance Perception Gap

Most people are not yet seeing what I'm seeing. To be sure, there are folks who are far ahead of me, but looking at the overall knowledge worker economy, I'm quite sure I'm in the top 1% or even 0.1% in terms of AI use in my daily work.

Since I'm heading up AI at a content marketing agency in tech, Animalz, I get to work with these models and tools every day, the whole day. Judging by how people respond to what I'm doing with AI, whether that's on LinkedIn or in conversations with friends, I estimate I'm 12-24 months ahead, even on many analysts and journalists.\*

Meanwhile, it's reasonable to assume the frontier AI companies are 12 to 24 months ahead of what publicly available, perhaps more. Models take time to be ready for _public_ release, and there's likely lots of fringe R&D happening inside these labs that allows them to see even further glimpses of the future.

If these rough assumptions are correct, the gap between the general public and what folks within the AI elite know and see is two to four years.

This perception gap might explain OpenAI's explosive growth: [revenue jumped from $13B ARR in July to $20B in November 2025](https://sacra.com/c/openai/)—a 54% increase in four months. To outside observers tracking 2023 capabilities, these numbers look absurd. But if you're experiencing what these tools can do _today_, that kind of growth starts to make sense. Enterprises aren't throwing money at hype; they're paying for tools that are already delivering measurable value.

While many people are still arguing (or laughing or worrying) about basic ChatGPT hallucinations, the OpenAI and Anthropic's of this world have — or can see a clear path to — AI agents capable of knowledge worker roles that currently go for $50k-$100k in annual salaries.

Again, this might sound crazy if you barely know something useful to do with ChatGPT, but from my vantage point, I don’t have to squint to see this reality.

Whether or not AI innovation progress slows down, or whether we hit AGI in 2027, 2035, or never, is irrelevant. Even if innovation progresses at a slower pace going forward (and it’s far from certain it will), people and organizations that catch up to where I am right now in my AI usage will be able to automate _at least_ 25% of their current work with the models we have _today_.

~~If a modest or extreme reality of human labor replacement by AI is what people inside AI companies and Silicon Valley’s elite are seeing, then that is what these crazy valuations are about.~~

When you’re building the technology and infrastructure that will capture, in the near future, a substantial share of the $4T annual that the USA spends on knowledge worker salaries\*, then these valuations and deals suddenly look realistic. In fact, it’s exactly what you would expect them to look like!

What you would then also expect is that not many people like to talk about this impending reality, and you’d be partially right.

Surprisingly, both Dareo Amodei and Sam Altman have touted this future.

Amodei has literally said massive job displacement is coming (most famously in Davos); Altman has hinted at a $2,000 a month AI agent for the enterprise. Both were met with scorn, skepticism, and laughter.

## Who has an answer for this Uncomfortable Truth?

Whether you put the probability of this future at 1% or 99%, nobody has a _good_ answer for how to deal with it when it does arrive.

Everyone, from Sam Altman to Bernie Sanders, likes to talk about universal basic income (UBI) as a solution. Give every person a very decent monthly income, no questions asked. The source of that money should either be the government or some kind of fund the AI companies put together from their profits.

Another popular one, especially among the AI elite, are references to previous economic revolutions when new, previously unimaginable jobs, always emerged. The same will surely happen now, they promise.\* Yet it’s completely unclear what such new types of work will be, nor when they’ll arrive.

There are even optimistic books like _Abundance_ and _Fully Automated Luxury Communism_ that propose complete and appealing overhauls of society, where nobody has to work, folks can pursue what they feel like, and all will be well.

Regardless of the soundness of such solutions even _in theory_, in practice nobody knows how we’ll really get from today to any of them. All of these ideas would need to clear practically impossible hurdles, from massive policy change to reforming capitalism itself\*.

Sadly, the _bad_ outcome of massive labor displacement is much easier to envision because it’s more realistic: our capitalist system driven by maximum shareholder gains more than happily replaces swats of human workers with AI, and there will be few rules, regulations, or other restrictions to stop that from happening\*.

## Bubbletime, and the living is easy

Besides its economic meaning, another dictionary definition for bubble includes:

> “A good or fortunate situation that is isolated from reality or unlikely to last.”

The bubble thesis is almost comforting in a way. "Don't worry, this is irrational exuberance, reality will reassert itself."

Though painful, we know how bubbles play out. Often, they even have benefits in the long-term\*.

For me personally, the bubble is relatively comfortable. I have my occasional worry about AI taking my job, but overall, on a day-to-day basis, I'm fine and enjoying my work with AI: building things I couldn't before, automating tedious tasks, and creating new, sometimes what feel like magical workflows.

But what should we do if the massive job displacement scenario is real? Start a revolution? Fight for equitable AI distribution? Resist adoption?

Maybe if I were in my twenties I would. But I'm in my forties with a family. I was there for the internet in 1994, and I know that individual resistance to genuinely transformative technology is futile.

Invest in the bubble? Become the person who implements AI for companies, automating other people's jobs to secure my own position?

Honestly? I don't know if I'd refuse that role if it came down to survival. And I think most people, if they're honest with themselves, would admit the same.

Hope for UBI? New jobs we can't imagine yet? That the AI elite will suddenly develop altruism? We can't even pass universal healthcare in the US. I'm not holding my breath for UBI.

What I _can_ say with confidence is this: the people dismissing AI's capabilities based on 2023 examples or minimal hands-on experience are making the same mistake as those who dismissed the internet in 1995. The technology is already far more capable than public perception suggests.

Whether that means we're in a bubble or witnessing the early stages of massive labor displacement, I can’t tell you with absolute certainty. But I know which possibility worries me most.

It's not the bubble.

—

Footnotes:

\*\* at the same stage yet with their AI usage.\*

_~~Many journalists and analysists seem either biases against the technology~~_ ~~or are barely using it,~~ Benedict Evans being the most prominent example, admitting on the Knowdge Project podcast that he barely uses AI. Others, like XXX in her book Empire of AI, use examples from 2022 and 2023 to make her points about the technology disappointing.\*

_Since these are the voices who for many normal folks who can’t really keep up provide the benchmark of what to think about this technology, it’s not strange that the overwhemling majority thinks the technology is not delivering on its promises._

\*\* I completely understand why writers and other creative folks decide to boycot the technology out of principle. But as a tech journalist or analyst, you have a responsibility in my opinion to use it as much as you can, just so you can have a reasonable assessment of its capabilities.\*

- (and perhaps much more, if you consider the global market and other industries that might get automated, like logistics or manufacturing)

\*\* It’s not like they haven’t. Dareo Amodei has said this is exactly what’s about to happen, and Sam Altman has already said a while back they expect to launch an agent for $2,000 per month that can work in enterprise companies.\*

\*\* even this, whether true or not, is a questionable desire, is it really better to go gfrom framing to factoritesd, or even from hunter gathering to agriculture?\*

- Or you need to believe in miracles, namely that altruisism will suddenly emerge in VCs, politicians, banks, and mutilnational CEOs. I mean, the USA can’t eveb agree on medical insurance for everyone, how would universal basic income ever pass?

\*\* The dotcom bubble provided massive amounts of fiber that came in handy after the bubble, and the proposed benefits of the current supposed AI one are massibe buildout of energy and perhaps, data centers (though those are much more “perishiable”).\*

\*\* which, sadly, is another reason why these valuations might actually make sense, because the path to massive labor replacement and the spoils have few barriers to overcome. Perhaps the biggest one is change management.\*

— —

### Scratchpad

This is exhilarating and unsettling in equal measure. I'm building things I couldn't before. I'm also becoming exactly the kind of person who could implement systems that displace other workers.

The alternative — massive labor displacement — has none of those characteristics. Few of us have ever experienced such a disruption of the job market, perhaps an event of this economical magnitude is unprecedented.

So what should we do? I honestly don't have the answer.

On a day-to-day basis, I'm enjoying my work with AI: building things I couldn't before, automating tedious tasks, and creating new, sometimes what feel like magical workflows.

And I do see new tasks and skills emerging in my own work that didn't exist a year ago. But I can't tell if these are glimpses of entirely new job categories that will replace what's lost, or just the privileged position of being early to a technology that will eventually automate me too.

Either way, I think the best strategy is mixing pragmatism with principles and empathy.

If you’re a knowledge worker, don’t stubbornly refuse a technology that is unlikely to go away. Use these tools extensively _now_, to understand what’s possible and what’s coming, what’s positive and what’s negative.

If you’re a business leader, consider how these tools can benefit both bottom line and well-being. Why not make more profit, improve the quality of your products or services, _and_ reduce people’s working hours.

If you're a policymaker or journalist shaping public understanding of AI, you have a responsibility to engage with the technology as it exists today, not as it existed in 2023, so you can shape stories and policies accordingly.

WHAT ABOUT REVOLUTION?

So what should we do if this is real? Start a revolution? Fight for equitable AI distribution? Resist adoption? Maybe if I were in my twenties I would. But I'm in my forties with a family. I was there for the internet in 1994, and I know that individual resistance to genuinely transformative technology is futile. Invest in the bubble? Become the person who implements AI for companies, automating other people's jobs to secure my own position? Honestly? I don't know if I'd refuse that role if it came down to survival. And I think most people, if they're honest with themselves, would admit the same. Hope for UBI? New jobs we can't imagine yet? That the AI elite will suddenly develop altruism? We can't even pass universal healthcare in the US. I'm not holding my breath for UBI."

Embrace AI and reap the benefits of being the automomater of others’ jobs for as long as that lasts?

Invest in the stock market and short the bubble?

Try to join an AI company?

Fight back and try to resist the usage of AI?

Overthrow the capitalist system?

Hope the prediction that some new jobs will appear is true?

Expect that these AI corporations and our governments will come together to really provide a basic universal income?

I honestly don’t have the answer. On a day to day basis, I’m enjoying tinkering with AI and honestly feel it’s allowing me to do and build things I couldn’t before.

But when I look up and see the forest for the trees, I am also concerned. To be clear: I’m not 100% convinced AI will destroy many jobs and no new ones will take it place. Again, in my own work, I see plenty of new tasks, skills, and opportunities appear that simply weren’t necessary or existent even one year ago. But whether those are really glimpses of entire new jobs that might appear, and whether those will be sufficient to replace the ones that will be lost, I find currently very hard to say.

if you’re the head of one of these AI companies, getting pushback on your valuation, do you really want to hammer much on the fact that you’re about to replace potentially millions of workers?

It’s classic case of damned if you do, damned if you don’t.

If you bring that argument, it gets written off as hype, fearmongering, evilness, or all three.

Sharing my own experience with current advanced AI models and tools like Claude Code and how people respond to what I share tells me that I’m about 12-24 months ahead of most other “normal” people in my AI usage.

Need to address/mention bias etc. somewhere.

Everyone's calling this an AI bubble. What if the scarier truth is that it's not?

The analysts point to the numbers: $560 billion invested, $35 billion in revenue. OpenAI valued at $500 billion with $8 billion in revenue. Nvidia's $100 billion circular financing deals. The math doesn't work. It's obviously a bubble.

Except.

I've been working with Claude Code all day, every day, for months now. And from what I'm seeing—just as a regular person with publicly available models—I could automate at least 25% of work at most companies I've looked at.

Not the creative work. That's still complicated.

The boring stuff.

Admin, finance, legal processing, project management, documentation, approvals. With current models, that stuff can be automated 80-90%.

And this is what's publicly released.

Here's what makes me uncomfortable: when analysts call this a bubble, they're measuring current revenue against current valuations. But what if these valuations aren't pricing current revenue? What if they're pricing near-term labor displacement?

How much do we spend on knowledge worker salaries globally? Trillions.

If the AI labs can see—with their internal models that are 12-24 months ahead of what we have access to—that they can start replacing significant chunks of knowledge work over the next few years, then $500 billion valuations aren't hype.

1. I’m writing “to an extend” because coding and data analysis seems to be one of those fields where “old” work and skills are rapidly being replaced by AI, but more work and new skills still seem to appear (i.e., there’s still mostly high demand for engineers right now).
2. Through MCP servers and APIs something which in itself I wouldn’t have been able to do without AI.