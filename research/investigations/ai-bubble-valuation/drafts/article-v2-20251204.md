# Let’s hope we’re in an AI bubble, the alternative is worse

Many analysts agree we're in an AI bubble. Here's how M.G. Siegler of Spyglass opened [a recent article](https://spyglass.org/ai-bubble/?ref=spyglass-newsletter):

> _"Yes, it's a bubble. Yes, that may be a good thing. Yes, it will hurt."_

Ben Thompson of Stratechery is [even more blunt](https://stratechery.com/2025/the-benefits-of-bubbles/): "_this_ is a bubble."

The current AI boom indeed checks all bubble boxes:

1. The companies involved are overvalued. (e.g., OpenAI at [$500B valuation](https://sacra.com/c/openai/) with $20B in annual recurring revenue.)
2. The deals they're making are crazy. (e.g., OpenAI signing [$1-1.5T in vendor deals](https://www.theregister.com/2025/11/04/the_circular_economy_of_ai/), often in [complex equity-for-service structures](https://ir.amd.com/financial-information/sec-filings/content/0001193125-25-230895/d28189d8k.htm).)
3. The current technology disappoints.

This last point, though often mentioned in passing, is actually the foundation of the bubble argument: **if AI _was_ delivering on its promise, the valuations and deals would be justified, so we wouldn't be in a bubble.**

## Does AI live up to its overhyped promise?

MIT NANDA found that 95% of generative AI investments have produced [no measurable returns](https://virtualizationreview.com/articles/2025/08/19/mit-report-finds-most-ai-business-investments-fail-reveals-genai-divide.aspx). Most companies outside of tech have anyway barely begun adoption: [only 9.7% of US firms](https://www.anthropic.com/research/anthropic-economic-index-september-2025-report) were using these tools as of August 2025. And it's easy to find published examples of the technology's gaffes: lawyers using hallucinated "evidence," images with too many fingers, and far worse examples of bias, racism, and other sins.

But what if those assessments of AI’s performance are incorrect, or, more precisely, _outdated_?

My AI journey has been [riddled with disappointment](https://www.animalz.co/blog/ai-addiction). But over the past three to six months, things have changed: workflows I dreamed up since ChatGPT’s release are finally working, often beyond my original wild visions for them.

For example, I'm working on an app for which Claude Code writes the code, corrects the UX design by using Chrome, and manages the project's tasks in Notion.

My role in the project is best described as orchestrator and perhaps taste maker. The AI is product manager, developer, and UX designer.

This pattern isn't unique to my side projects. When I consult with companies I see the same potential: at least 25% of routine knowledge work could be done by AI. This includes tasks in finance, HR, operations, and, to an extend, coding and data analysis.[1](#fn:1) Soon, I believe AI can also handle a significant part of work done by middle managers.[2](#fn:2)

This assessment is not based on forecasts or claims by others. I’m experiencing first-hand what a capable model (Sonnet 4.5) combined with unique data sources[3](#fn:3) and great tools (e.g., Claude Code) can do.

### Give me examples of actual jobs and tasks that can be replaced (and how)

![](https://lex-img-p.s3.us-west-2.amazonaws.com/img/55f4ca12-b9e9-4ec4-9b7f-bfbff079b597-RackMultipart20251124-155-6byljw.png)

The idea of 25+% task replacement got a fair and direct challenge when I floated it [on LinkedIn](https://www.linkedin.com/posts/metztim_everyones-calling-this-an-ai-bubble-i-hope-activity-7396801059056340992-83Ns?utm_source=share&utm_medium=member_desktop&rcm=ACoAAADtk0sB66vemUuAUdT2uD1H6hp71UW8xH4): **name a specific mid-level role that can be replaced soon.** That question made me think through my claim more carefully, and I realized it needs more context, nuance, and detail.

#### 1\. It’s about context engineering, not ChatGPT access and training

The AI replacement I envision doesn’t happen from giving folks ChatGPT subscriptions and a training. I’ve changed my beliefs over what’s possible with AI mainly because of one concept: _context engineering_.

The latest AI models start to deliver the goods when they have context, meaning access to relevant data sources, and lots of them. Think meeting transcripts, strategy docs, product boards — whatever your human workers also need to do their work well.

To get such context and replace significant amounts of work within modern knowledge organizations requires a specialist (or a team of them, depending on the size of your org), to come in and build an AI-native infrastructure for the company. Such an overhaul could take anywhere from a few weeks for a small startup or SMB to six months or more for a large enterprise.

#### 2\. I’m not talking about product managers, writers, designers, or other creative work

Because I often write about content, creativity, and AI, it’s understandable to assume my thesis is about such work, but it isn’t.

To me, such jobs are at least at the upper spectrum of "mid-level." What I'm really thinking of is **process-driven, rule-based work**: tasks with clear inputs and outputs, coordination and scheduling functions, and information management roles. Think bookkeeping, executive assistance, operations coordination—work that follows documentable processes and requires judgment within defined parameters.

That’s also why I expect a lot of mid-level management work to soon be replaceable: it involves relatively low-stakes decision-making, coaching, evaluation, and planning. These are all tasks the latest AI models can handle given sufficient context.

#### 3\. I’m thinking of the entire knowledge work sector, not just tech

Technology companies employ about 25% of knowledge workers. If you look at the entire knowledge work economy, many companies outside of the tech industry are much further behind on AI than most of you reading this article.

It's easy to lose perspective of this, but there are large numbers of people doing knowledge work tasks in low-tech companies that can easily be automated.

Some examples from a variety of sectors: processing prior authorizations and insurance verifications in healthcare, reviewing discovery documents and generating research memos in legal firms, reconciling vendor invoices and generating variance reports in manufacturing, verifying loan applications and investigating fraud alerts in financial services, and creating property listings and lease agreement drafts in real estate.

#### 4\. Whether 25% of roles or 25% of tasks get replaced doesn’t really matter

I do think there are complete roles that can be automated based on the types of work we just discussed: executive assistants, junior accountants, data entry specialists, and many administrative coordinators. But the distinction between role and task replacement is largely semantic.

Even if the amount of automation is divided across skills instead of complete roles, it will ultimately lead to lay offs or fewer people being hired (unless, completely new jobs or work appears for those folks, but we’ll get to that later).

#### Examples

Klarna provides a concrete example. The fintech reduced headcount from 5,527 to ~2,900 employees (47% reduction) while more than doubling revenue. Its AI chatbot handles work equivalent to 700 full-time customer service agents. Marketing staff dropped from 200 to 100 while running more campaigns, with AI handling 80% of copywriting at 70% lower costs. CEO Sebastian Siemiatkowski plans to reduce to 2,000 total employees.

This isn't isolated. Amazon eliminated 14,000 managerial positions in October 2025, explicitly linking the restructuring to AI investment. Meta cut 3,600 employees with Zuckerberg stating he doesn't want "management structure that's just managers managing managers." Salesforce CEO Marc Benioff went from saying AI "augments people" in July to "I've reduced it from 9,000 heads to about 5,000" by September.

## The Four Year AI Performance Perception Gap

Most people are not yet seeing what I'm seeing. Since I head up AI at [a content marketing agency for b2b tech companies](https://animalz.co/), I get to work with these models and tools every day, the whole day. Judging by how people respond to what I'm doing with AI, whether that's on LinkedIn or in conversations with friends, I estimate I'm 12-24 months ahead, even on many analysts and journalists.[4](#fn:4)

Meanwhile, it's reasonable to assume the frontier AI companies are 12 to 24 months ahead of what’s publicly available, perhaps more. Models take time to be ready for _public_ release, and there's lots of fringe R&D happening inside these labs that allows them to estimate what the future looks like.[5](#fn:5)

If these rough assumptions are correct, the gap between the general public and what folks within the AI elite know and see is two to four years.

Dario Amodei, Anthropic's CEO, recently said he believes we're on track for AI systems that surpass humans "in every task within two to three years." That's January 2027 or 2028. Sam Altman has made all kinds of claims about super intelligence, and also commercial products like a $2,000 a month agent for the enterprise.

Many other folks within tech and Silicon Valley are talking about similar capabilities and timelines for AI being able to do knowledge work. Investor Andrew Wilkinson recently warned that we have roughly 918 days "until human brains turn into vinyl:" beautiful and unique, but obsolete for most practical purposes.

His timeline: 2026-27 sees widespread automation, 2028-29 brings mass white-collar displacement, and by 2030+ we're scrambling for solutions like UBI while entirely new economic realities emerge.

While many people are still arguing (or laughing or worrying) about basic ChatGPT hallucinations, what if the Silicon Valley elite have — or can see — a clear path to AI agents capable of knowledge worker roles that currently go for $50k-$100k in annual salaries?

Again, this might sound crazy if you barely know something useful to do with ChatGPT, but from my vantage point, I don’t have to squint anymore to see this reality.

Whether or not AI innovation progress slows down, or whether we hit AGI in 2027, 2035, or never, is irrelevant. Even if innovation progresses at a slower pace going forward (and it’s far from certain it will), people and organizations that catch up to where I am right now in my AI usage will be able to automate _at least_ 25% of their current work with the models we have _today_.

### But what about all the naysayers?

There are three categories of people saying AI is not delivering as promised. Since my argument is built on the premise that it does, I'd like to examine why each group might be missing what I'm seeing.

#### 1\. Burned Casual Consumers

Because of a few disappointing experiences with AI, often quite some time ago, some people write off the whole technology as useless hype. They typically don’t have an obvious use case for AI in their work or life, and their skepticism is strengthened by counter-arguments they hear from folks in the other two categories.

#### 2\. Principled Opposers

People who strongly oppose the current AI products and development on principles. These reasons cover a whole spectrum but can include everything from environmental issues (e.g, extreme energy usage by AI services), moral concerns (e.g., copyright infringement, abuse of ultra low-wage workers in other countries for data labelling), and existential risk (e.g., AI wiping out most of humanity).

#### 3\. Knowledge-Cursed Experts

AI researchers, AI engineers, tech analysts, and other highly competent technical experts who dismiss the capabilities of AI and particularly generative AI.

This last category deservers extra attention because they’re the most credible judges of the technology. Unlike me and 99.9% of other people, they have a far better understanding of how generative AI really works.

I’d sum up their main argument as follows: generative AI is simplistic and unreliable. It’s just predicting the next token (character), therefore this technology isn’t intelligent and never will be.

What I think is going on here is the curse of knowledge: _because_ they understand what’s happening under the hood, and because they know of far superior (technical) approaches to AI, they’re dismissive of generative AI.

This bias creates a blindspot for them that people like me don’t have: I don’t care how it works under the hood, I can see it actually working and delivering meaningful results.

> "It doesn't matter whether a cat is black or white, as long as it catches mice." — Deng Xiaoping

People in any of these categories will typically also say something along the lines of “it’s been three years since ChatGPT, and AI adoption (at companies) is low or even regressing.” The problem with this argument in the light of my thesis is that 1) only recently have models really become capable of doing valuable knowledge work and 2) humans take time adapt and change their knowledge, habits, and organizations. It’s a bit like being in 1997 and saying that The Internet is hype because people are only making online versions of their printed brochures. That’s not so much the problem of the technology as of people not having grasped yet what the new technology makes possible. The same is going on here.

## The Math of Massive Displacement

Say 25% or more of current knowledge work can indeed be done by AI within the next two to four years, does the math of my argument add up? In other words, would such replacement and its timeline justify the valuations we’re seeing of AI-related stocks and companies?

That question is trickier to answer than one might imagine. Valuations aren’t literally based on traders and investors wringing their hands at all the jobs they’re going to replace with capital (at least not in public). Most rationales are about the demand for NVIDIA chips and their scarcity, the revenue growth of the main frontier labs, and the projected revenue versus costs per square meter of data centers.

Yet indirectly, these are the metrics and infrastructure that represent the “country of a million geniuses in a data center” that Dario Amodei predicts are coming.

So, to figure out through cold calculations and rigid rationale whether my argument makes sense I turned to, AI, in the form of Claude Code.

\[Figure out and insert calculation here.\]

## **SECTION: "The Bubble and the Displacement Are Intertwined" (or similar title)**

**Opening hook**: Most people treat the bubble question and the displacement question as opposites—either valuations are irrational or AI will transform work. But what if they're not mutually exclusive?

**Main argument - The paradox**:

- Scenario 1: If displacement accelerates faster than expected → validates the valuations, prevents bubble from popping (revenue materializes to justify the numbers)
- Scenario 2: If bubble pops → creates economic pressure that could _accelerate_ displacement (companies forced to cut costs will finally implement what's already technically possible)

**Critical distinction from dot-com** (this is your strongest material):

- Dot-com bubble: Vision exceeded infrastructure. [Pets.com](http://Pets.com) failed because logistics, payments, broadband weren't ready. When bubble popped, you had to wait years for technology to catch up.
- AI situation: Technology is ready NOW, but perception and adoption lag. When economic pressure hits, there's no "waiting for infrastructure" period—companies just have to implement what already exists.
- Pull in the Klarna, Amazon, Meta examples from below to show this is already happening at early-adopter companies

**The speed factor** (make this explicit):

- This could happen over 5 years, not 20 years
- That's fundamentally different from previous technological transitions
- Historical "new jobs always emerge" precedent assumes gradual change that allows labor markets to adapt
- If displacement is compressed into a shorter timeline, those reassuring historical patterns might not apply

**End on the uncomfortable implication**: Whether we're in a bubble or not might be the wrong question. Both paths could lead to the same destination—just with different timelines.

* * *

## **SECTION: Counter-Arguments and Alternative Outcomes**

**Frame this section**: These are the main reasons people think massive displacement won't happen. Some are plausible. None are certain enough to be reassuring.

**1\. Autor's Demographic Argument** (give this serious weight):

- Pull his quote from below: "The industrialized world is awash in jobs, and it's going to stay that way"
- His point: All people who'll turn 30 in 2053 have already been born. Barring massive immigration changes, rich countries will run out of workers before running out of jobs
- His research: 60% of 2018 employment was in job titles that didn't exist in 1940
- Why you still worry despite this being plausible: Even if long-term job numbers stabilize, the transition period could be brutal. And the _type_ of work available may look nothing like what displaced workers trained for. A lawyer becoming an HVAC technician isn't a smooth transition even if both are "jobs."

**2\. The Trades Counter-Argument** (you already have great material on this):

- Pull the Wilkinson quote and trades flooding section from below
- The jobs survive, but margins collapse
- "Learn to code" was glib; "learn a trade" might not be the safe harbor it appears

**3\. New Jobs Will Emerge** (acknowledge but remain skeptical):

- This is what always happens historically
- You personally see new tasks in your own work that didn't exist a year ago
- But: Are these glimpses of entirely new job categories, or just temporary positions until AI catches up?
- The speed factor again: If displacement happens over 5 years instead of 50, new jobs might not emerge fast enough

**4\. UBI and Policy Solutions** (brief, honest dismissal):

- Everyone from Altman to Sanders mentions UBI
- Pull your line from below: "We can't even pass universal healthcare in the US. I'm not holding my breath for UBI."
- The political/economic hurdles are practically insurmountable in the timeframe we're discussing

**Transition to conclusion**: These alternatives are possible. But betting your livelihood on them feels like hoping for the best rather than preparing for the plausible.

* * *

## **CONCLUSION: "Bubbletime, and the living is easy"**

**Opening with the alternate definition of bubble**:

- Pull the dictionary definition from below: "A good or fortunate situation that is isolated from reality or unlikely to last"
- The bubble thesis is almost comforting—we know how those play out

**Personal honesty section**:

- Pull from below: Your day-to-day experience—building things you couldn't before, enjoying the work, magical workflows
- You're in your forties with a family
- The ambiguous position: Would you refuse to be the person implementing AI systems that displace others if it came to survival? Most people wouldn't if they're honest.
- Pull the Wilkinson quote about sand castles and tides

**The core tension**:

- You can't say with certainty which future we're in
- But you know which possibility worries you most
- Pull final line from below: "It's not the bubble."

**Possible addition** (depending on tone): Brief nod to what readers should do—not prescriptive, but something like: "Use the tools extensively now to understand what's possible. Don't dismiss based on 2023 experiences. The technology is already more capable than public perception suggests."

* * *

**Notes on what to pull from the scratchpad/lower sections**:

- The Klarna example and other company data (Amazon, Meta, Salesforce cuts)
- Wilkinson's specific quotes about vinyl/918 days and sand castles
- The trades flooding economic argument
- Autor's demographic argument
- Your personal positioning about being 40s with family
- The "it doesn't matter if a cat is black or white" Deng Xiaoping quote could go in the Knowledge-Cursed Experts section
- Most of the UBI/revolution/philosophical wandering can be cut or condensed into brief mentions

* * *

* * *

~~If a modest or extreme reality of human labor replacement by AI is what people inside AI companies and Silicon Valley’s elite are seeing, then that is what these crazy valuations are about.~~

~~When you’re building the technology and infrastructure that will capture, in the near future, a substantial share of the $4T annual that the USA spends on knowledge worker salaries\*, then these valuations and deals suddenly look realistic. In fact, it’s exactly what you would expect them to look like!~~

~~The efficiency gains are already measurable. AI-native companies like Midjourney ($500M ARR with 40-100 employees) and Cursor ($1B ARR with 60-150 employees) achieve 5-7x higher revenue-per-employee than traditional SaaS companies. This isn't hype about the future—it's measurable performance today.~~

~~What you would then also expect is that not many people like to talk about this impending reality, and you’d be partially right.~~

~~Surprisingly, both Dareo Amodei and Sam Altman have touted this future.~~

~~Amodei has literally said massive job displacement is coming (most famously in Davos); Altman has hinted at a $2,000 a month AI agent for the enterprise. Both were met with scorn, skepticism, and laughter.~~

## Who has an answer for this Uncomfortable Truth?

Whether you put the probability of this future at 1% or 99%, nobody has a _good_ answer for how to deal with it when it does arrive.

Everyone, from Sam Altman to Bernie Sanders, likes to talk about universal basic income (UBI) as a solution. Give every person a very decent monthly income, no questions asked. The source of that money should either be the government or some kind of fund the AI companies put together from their profits.

The most sophisticated counter-argument comes from MIT economist David Autor: "The industrialized world is awash in jobs, and it's going to stay that way." His demographic point is simple but powerful—all people who will turn thirty in 2053 have already been born. Barring massive immigration changes, rich countries will run out of workers before running out of jobs. His research shows 60% of 2018 employment was in job titles that didn't exist in 1940.

This offers a genuinely plausible alternative outcome: chronic labor shortages may absorb displaced workers faster than AI can replace them. It's the kind of counter-argument that deserves serious consideration rather than dismissal.

### But What About the Trades?

Even if Autor is right and jobs survive, there's another problem: the economics might not.

Right now, trades like HVAC, plumbing, electrical work, and home services are profitable for one reason: limited supply. There aren't enough qualified technicians or entrepreneurs in these fields. High demand, low supply—business owners capture the spread.

But where do millions of laid-off knowledge workers go?

Investor Andrew Wilkinson articulated the cascade effect: Think about the people who followed society's blueprint perfectly—top universities, crushing student debt, prestigious internships, climbing the corporate ladder. The MBAs, consultants, middle managers, corporate lawyers who spent a decade in school, accountants who collected every certification. They did everything right. And suddenly, they're holding worthless credentials in industries that no longer need humans. These millions of educated, ambitious people aren't just going to disappear. They're going to pivot hard into whatever fields they think AI can't touch, flooding blue-collar work with their education and capital.

This disruption takes time—retraining as an HVAC technician doesn't happen overnight—but the flood seems inevitable. And when it comes, margins collapse.

The jobs themselves will survive. Increased efficiency might even drive higher demand (Jevons Paradox): cheaper services mean more consumption. Maybe you'll finally build that outdoor kitchen, get weekly massages instead of monthly ones, hire regular cleaning services instead of doing it yourself.

But business owners won't see the same profits. More competition means better prices for consumers but razor-thin margins for businesses. Just like restaurants, hair salons, and convenience stores today—industries where intense competition creates a brutal reality: long hours, thin margins for owners, modest wages for workers despite the essential nature of their services.

The "learn to code" advice was always glib. But "learn a trade" might not be the safe harbor it appears either.

Another popular one, especially among the AI elite, are references to previous economic revolutions when new, previously unimaginable jobs, always emerged. The same will surely happen now, they promise.\* Yet it's completely unclear what such new types of work will be, nor when they'll arrive.

There are even optimistic books like _Abundance_ and _Fully Automated Luxury Communism_ that propose complete and appealing overhauls of society, where nobody has to work, folks can pursue what they feel like, and all will be well.

Regardless of the soundness of such solutions even _in theory_, in practice nobody knows how we’ll really get from today to any of them. All of these ideas would need to clear practically impossible hurdles, from massive policy change to reforming capitalism itself\*.

Sadly, the _bad_ outcome of massive labor displacement is much easier to envision because it’s more realistic: our capitalist system driven by maximum shareholder gains more than happily replaces swats of human workers with AI, and there will be few rules, regulations, or other restrictions to stop that from happening\*.

![](https://lex-img-p.s3.us-west-2.amazonaws.com/img/1dd012b0-3fa5-487d-b030-72a5657394d2-RackMultipart20251124-215-er68ge.png)

![](https://lex-img-p.s3.us-west-2.amazonaws.com/img/cf3b1543-b9a3-4c80-9102-1eb19a6267f7-RackMultipart20251124-186-53qz94.png)

## Bubbletime, and the living is easy

This might be a better conclusion:

Besides its economic meaning, another dictionary definition for bubble includes:

> "A good or fortunate situation that is isolated from reality or unlikely to last."

The bubble thesis is almost comforting in a way. "Don't worry, this is irrational exuberance, reality will reassert itself."

Though painful, we know how bubbles play out. Often, they even have benefits in the long-term\*.

For me personally, the bubble is relatively comfortable. I have my occasional worry about AI taking my job, but overall, on a day-to-day basis, I'm fine and enjoying my work with AI: building things I couldn't before, automating tedious tasks, and creating new, sometimes what feel like magical workflows.

But what should we do if the massive job displacement scenario is real? Start a revolution? Fight for equitable AI distribution? Resist adoption?

Maybe if I were in my twenties I would. But I'm in my forties with a family. I was there for the internet in 1994, and I know that individual resistance to genuinely transformative technology is futile.

As an investor, Andrew Wilkinson put it this way: "I feel like I'm evaluating sand castles on a beach with an unpredictable tide. Some castles are built higher than others. Some will survive. But the tide is far less predictable than it was last decade."

That's exactly how it feels. We're all building our sand castles—our careers, our businesses, our expertise—and the tide is coming. We just don't know how high it will reach or how fast.

Invest in the bubble? Become the person who implements AI for companies, automating other people's jobs to secure my own position?

Honestly? I don't know if I'd refuse that role if it came down to survival. And I think most people, if they're honest with themselves, would admit the same.

Hope for UBI? New jobs we can't imagine yet? That the AI elite will suddenly develop altruism? We can't even pass universal healthcare in the US. I'm not holding my breath for UBI.

What I _can_ say with confidence is this: the people dismissing AI's capabilities based on 2023 examples or minimal hands-on experience are making the same mistake as those who dismissed the internet in 1995. The technology is already far more capable than public perception suggests.

Whether that means we're in a bubble or witnessing the early stages of massive labor displacement, I can't tell you with absolute certainty. But I know which possibility worries me most.

It's not the bubble.

## Bubbletime, and the living is easy

This might be a better conclusion:

![](https://lex-img-p.s3.us-west-2.amazonaws.com/img/3bf94098-7224-4417-a440-79021ffc75df-RackMultipart20251124-200-ty9dvj.png)

Besides its economic meaning, another dictionary definition for bubble includes:

> “A good or fortunate situation that is isolated from reality or unlikely to last.”

The bubble thesis is almost comforting in a way. "Don't worry, this is irrational exuberance, reality will reassert itself."

Though painful, we know how bubbles play out. Often, they even have benefits in the long-term\*.

For me personally, the bubble is relatively comfortable. I have my occasional worry about AI taking my job, but overall, on a day-to-day basis, I'm fine and enjoying my work with AI: building things I couldn't before, automating tedious tasks, and creating new, sometimes what feel like magical workflows.

But what should we do if the massive job displacement scenario is real? Start a revolution? Fight for equitable AI distribution? Resist adoption?

Maybe if I were in my twenties I would. But I'm in my forties with a family. I was there for the internet in 1994, and I know that individual resistance to genuinely transformative technology is futile.

Invest in the bubble? Become the person who implements AI for companies, automating other people's jobs to secure my own position?

Honestly? I don't know if I'd refuse that role if it came down to survival. And I think most people, if they're honest with themselves, would admit the same.

Hope for UBI? New jobs we can't imagine yet? That the AI elite will suddenly develop altruism? We can't even pass universal healthcare in the US. I'm not holding my breath for UBI.

What I _can_ say with confidence is this: the people dismissing AI's capabilities based on 2023 examples or minimal hands-on experience are making the same mistake as those who dismissed the internet in 1995. The technology is already far more capable than public perception suggests.

Whether that means we're in a bubble or witnessing the early stages of massive labor displacement, I can’t tell you with absolute certainty. But I know which possibility worries me most.

It's not the bubble.

![](https://lex-img-p.s3.us-west-2.amazonaws.com/img/a4ca3a8f-7edd-43e3-b019-ac888bdb7d41-RackMultipart20251124-200-7pkze0.png)

—

Footnotes:

\*\* at the same stage yet with their AI usage.\*

_~~Many journalists and analysists seem either biases against the technology~~_ ~~or are barely using it,~~ \*

_Since these are the voices who for many normal folks who can’t really keep up provide the benchmark of what to think about this technology, it’s not strange that the overwhemling majority thinks the technology is not delivering on its promises._

\*\* I completely understand why writers and other creative folks decide to boycot the technology out of principle. But as a tech journalist or analyst, you have a responsibility in my opinion to use it as much as you can, just so you can have a reasonable assessment of its capabilities.

- (and perhaps much more, if you consider the global market and other industries that might get automated, like logistics or manufacturing)

\*\* It’s not like they haven’t. Dareo Amodei has said this is exactly what’s about to happen, and Sam Altman has already said a while back they expect to launch an agent for $2,000 per month that can work in enterprise companies.\*

\*\* even this, whether true or not, is a questionable desire, is it really better to go gfrom framing to factoritesd, or even from hunter gathering to agriculture?\*

- Or you need to believe in miracles, namely that altruisism will suddenly emerge in VCs, politicians, banks, and mutilnational CEOs. I mean, the USA can’t eveb agree on medical insurance for everyone, how would universal basic income ever pass?

\*\* The dotcom bubble provided massive amounts of fiber that came in handy after the bubble, and the proposed benefits of the current supposed AI one are massibe buildout of energy and perhaps, data centers (though those are much more “perishiable”).\*

\*\* which, sadly, is another reason why these valuations might actually make sense, because the path to massive labor replacement and the spoils have few barriers to overcome. Perhaps the biggest one is change management.\*

— —

### Scratchpad

This is exhilarating and unsettling in equal measure. I'm building things I couldn't before. I'm also becoming exactly the kind of person who could implement systems that displace other workers.

The alternative — massive labor displacement — has none of those characteristics. Few of us have ever experienced such a disruption of the job market, perhaps an event of this economical magnitude is unprecedented.

So what should we do? I honestly don't have the answer.

On a day-to-day basis, I'm enjoying my work with AI: building things I couldn't before, automating tedious tasks, and creating new, sometimes what feel like magical workflows.

And I do see new tasks and skills emerging in my own work that didn't exist a year ago. But I can't tell if these are glimpses of entirely new job categories that will replace what's lost, or just the privileged position of being early to a technology that will eventually automate me too.

Either way, I think the best strategy is mixing pragmatism with principles and empathy.

If you’re a knowledge worker, don’t stubbornly refuse a technology that is unlikely to go away. Use these tools extensively _now_, to understand what’s possible and what’s coming, what’s positive and what’s negative.

If you’re a business leader, consider how these tools can benefit both bottom line and well-being. Why not make more profit, improve the quality of your products or services, _and_ reduce people’s working hours.

If you're a policymaker or journalist shaping public understanding of AI, you have a responsibility to engage with the technology as it exists today, not as it existed in 2023, so you can shape stories and policies accordingly.

WHAT ABOUT REVOLUTION?

So what should we do if this is real? Start a revolution? Fight for equitable AI distribution? Resist adoption? Maybe if I were in my twenties I would. But I'm in my forties with a family. I was there for the internet in 1994, and I know that individual resistance to genuinely transformative technology is futile. Invest in the bubble? Become the person who implements AI for companies, automating other people's jobs to secure my own position? Honestly? I don't know if I'd refuse that role if it came down to survival. And I think most people, if they're honest with themselves, would admit the same. Hope for UBI? New jobs we can't imagine yet? That the AI elite will suddenly develop altruism? We can't even pass universal healthcare in the US. I'm not holding my breath for UBI."

Embrace AI and reap the benefits of being the automomater of others’ jobs for as long as that lasts?

Invest in the stock market and short the bubble?

Try to join an AI company?

Fight back and try to resist the usage of AI?

Overthrow the capitalist system?

Hope the prediction that some new jobs will appear is true?

Expect that these AI corporations and our governments will come together to really provide a basic universal income?

I honestly don’t have the answer. On a day to day basis, I’m enjoying tinkering with AI and honestly feel it’s allowing me to do and build things I couldn’t before.

But when I look up and see the forest for the trees, I am also concerned. To be clear: I’m not 100% convinced AI will destroy many jobs and no new ones will take it place. Again, in my own work, I see plenty of new tasks, skills, and opportunities appear that simply weren’t necessary or existent even one year ago. But whether those are really glimpses of entire new jobs that might appear, and whether those will be sufficient to replace the ones that will be lost, I find currently very hard to say.

if you’re the head of one of these AI companies, getting pushback on your valuation, do you really want to hammer much on the fact that you’re about to replace potentially millions of workers?

It’s classic case of damned if you do, damned if you don’t.

If you bring that argument, it gets written off as hype, fearmongering, evilness, or all three.

Sharing my own experience with current advanced AI models and tools like Claude Code and how people respond to what I share tells me that I’m about 12-24 months ahead of most other “normal” people in my AI usage.

Need to address/mention bias etc. somewhere.

Everyone's calling this an AI bubble. What if the scarier truth is that it's not?

The analysts point to the numbers: $560 billion invested, $35 billion in revenue. OpenAI valued at $500 billion with $8 billion in revenue. Nvidia's $100 billion circular financing deals. The math doesn't work. It's obviously a bubble.

Except.

I've been working with Claude Code all day, every day, for months now. And from what I'm seeing—just as a regular person with publicly available models—I could automate at least 25% of work at most companies I've looked at.

Not the creative work. That's still complicated.

The boring stuff.

Admin, finance, legal processing, project management, documentation, approvals. With current models, that stuff can be automated 80-90%.

And this is what's publicly released.

Here's what makes me uncomfortable: when analysts call this a bubble, they're measuring current revenue against current valuations. But what if these valuations aren't pricing current revenue? What if they're pricing near-term labor displacement?

How much do we spend on knowledge worker salaries globally? Trillions.

If the AI labs can see—with their internal models that are 12-24 months ahead of what we have access to—that they can start replacing significant chunks of knowledge work over the next few years, then $500 billion valuations aren't hype.

1. I’m writing “to an extend” because coding and data analysis seems to be one of those fields where “old” work and skills are rapidly being replaced by AI, but more work and new skills still seem to appear (i.e., there’s still mostly high demand for engineers right now).

McKinsey research shows 49% of managerial work could be automated, with less than 30% of middle managers' time spent on actual people leadership: roughly 75% goes to individual execution or administrative tasks that AI handles well. Gartner projects that by 2026, 20% of organizations will eliminate more than half of their middle management positions using AI.

Through MCP servers and APIs something which in itself I wouldn’t have been able to do without AI.

4. Benedict Evans being the most prominent example, admitting on the Knowledge Project podcast that he barely uses AI. Others, like XXX in her book Empire of AI, use examples from 2022 and 2023 to make her points about the technology disappointing.
5. I’m not suggesting these labs are two versions ahead in terms of fully completed models, but they surely have research and experiments going on that give them a line of sight at what’s to come that’s not available to the public.